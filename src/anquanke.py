# -*- coding: utf-8 -*-
import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from model.news import News
from db.news_db import NewsDB

class Anquanke:
    def __init__(self):
        self.url = 'https://www.anquanke.com/'
        self.headers = {
            "User-Agent": "Mozilla/5.0"
        }
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        self.db_path = os.path.join(self.base_dir, "..", "data", "news.db")
        self.db = NewsDB(path=self.db_path)

    def scrape(self):
        news_list = []

        try:
            res = requests.get(self.url, headers=self.headers, timeout=10)
            res.raise_for_status()
        except requests.RequestException as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return []

        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select("div._94 li.item")

        if not items:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ç« é¡¹ ._94 li.item")
            return []

        print(f"ğŸ“¦ å…±æ‰¾åˆ° {len(items)} ç¯‡æ–‡ç« ")

        for item in items:
            try:
                title_tag = item.select_one("div.item-main div.title a")
                if not title_tag:
                    print("âš ï¸ è·³è¿‡ï¼šæœªæ‰¾åˆ°æ ‡é¢˜")
                    continue

                title = title_tag.get_text(strip=True)
                url = urljoin(self.url, title_tag.get("href", "#"))

                date = "æœªçŸ¥"

                news = News(
                    title=title,
                    url=url,
                    date=date,
                    source="å®‰å…¨å®¢ Anquanke"
                )

                self.db.insert_news(news)
                news_list.append(news)
                print(f"âœ… æˆåŠŸå†™å…¥: {title} [{date}]")

            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡ç« å‡ºé”™: {e}")
                continue

        return news_list
